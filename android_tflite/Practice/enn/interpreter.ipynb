{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "default_model = \"\"\n",
    "samsung_model = \"./yolo_v4_tiny_float/samsung_yolo_v4_tiny_float.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = samsung_model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== input tensor information ==\n",
      "input tensor 0:\n",
      "\tname: input_1\n",
      "\tshape: [  1 416 416   3]\n",
      "\tdtype: <class 'numpy.float32'>\n",
      "\n",
      "== output tensor information ==\n",
      "output tensor 0:\n",
      "\tname: tf_op_layer_concat_15/concat_15\n",
      "\tshape: [   1 2535    4]\n",
      "\tdtype: <class 'numpy.float32'>\n",
      "\n",
      "output tensor 1:\n",
      "\tname: tf_op_layer_concat_16/concat_16\n",
      "\tshape: [   1 2535   80]\n",
      "\tdtype: <class 'numpy.float32'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 입력 및 출력 정보 얻기\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# 입력 텐서 정보 출력\n",
    "print(\"== input tensor information ==\")\n",
    "for i, detail in enumerate(input_details):\n",
    "    print(f\"input tensor {i}:\")\n",
    "    print(f\"\\tname: {detail['name']}\")\n",
    "    print(f\"\\tshape: {detail['shape']}\")\n",
    "    print(f\"\\tdtype: {detail['dtype']}\")\n",
    "    print()\n",
    "\n",
    "# 출력 텐서 정보 출력\n",
    "print(\"== output tensor information ==\")\n",
    "for i, detail in enumerate(output_details):\n",
    "    print(f\"output tensor {i}:\")\n",
    "    print(f\"\\tname: {detail['name']}\")\n",
    "    print(f\"\\tshape: {detail['shape']}\")\n",
    "    print(f\"\\tdtype: {detail['dtype']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_path = \"./images/bird-8047149_1280.jpg\"\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image = image.resize((416, 416))\n",
    "input_data = np.array(image, dtype=np.float32)\n",
    "input_data = input_data / 255.0\n",
    "input_data = np.expand_dims(input_data, axis=0)\n",
    "# input_data = np.expand_dims(input_data / 255.0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m output_boxes \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m output_classes \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m output_scores \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(\u001b[43moutput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m output_data \u001b[38;5;241m=\u001b[39m [output_boxes, output_scores, output_classes]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "output_data = [output_boxes, output_scores, output_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image_path, input_size):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize(([input_size[1], input_size[0]]))\n",
    "    image = np.array(image, dtype=np.float32)\n",
    "    image = image / 255.0  # YOLO는 일반적으로 [0, 1] 범위로 정규화\n",
    "    image = np.expand_dims(image, axis=0)  # 배치 차원 추가\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 이미지 후처리 함수\n",
    "def process_output(output_data, input_image, threshold=0.5):\n",
    "    boxes, classes = output_data\n",
    "    boxes = boxes[0]\n",
    "    # scores = scores[0]\n",
    "    classes = classes[0]\n",
    "    \n",
    "    # 입력 이미지 크기 가져오기\n",
    "    input_h, input_w, _ = input_image.shape\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] > threshold:\n",
    "            # 바운딩 박스 그리기\n",
    "            ymin, xmin, ymax, xmax = boxes[i]\n",
    "            xmin = int(xmin * input_w)\n",
    "            xmax = int(xmax * input_w)\n",
    "            ymin = int(ymin * input_h)\n",
    "            ymax = int(ymax * input_h)\n",
    "            \n",
    "            cv2.rectangle(input_image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            label = f\"{int(classes[i])}: {scores[i]:.2f}\"\n",
    "            cv2.putText(input_image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[416 416]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'scores' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m original_image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(original_image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 감지된 객체 후처리 및 시각화\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_image_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 결과 이미지 출력\u001b[39;00m\n\u001b[0;32m     46\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m, in \u001b[0;36mprocess_output\u001b[1;34m(output_data, input_image, threshold)\u001b[0m\n\u001b[0;32m      5\u001b[0m boxes, classes \u001b[38;5;241m=\u001b[39m output_data\n\u001b[0;32m      6\u001b[0m boxes \u001b[38;5;241m=\u001b[39m boxes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m classes \u001b[38;5;241m=\u001b[39m classes[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 입력 이미지 크기 가져오기\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'scores' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOLOv4 TFLite 모델 경로\n",
    "model_path = samsung_model\n",
    "\n",
    "# 모델 불러오기\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# 입력 및 출력 텐서 정보 얻기\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# 모델 입력 크기 (보통 YOLOv4는 416x416 또는 608x608)\n",
    "input_shape = input_details[0]['shape'][1:3]\n",
    "\n",
    "print(input_shape)\n",
    "# 이미지 전처리\n",
    "input_data = preprocess_image(image_path, input_shape)\n",
    "\n",
    "# 입력 텐서 설정\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# 추론 실행\n",
    "interpreter.invoke()\n",
    "\n",
    "# 출력 텐서 가져오기\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_data = [output_boxes, output_classes]\n",
    "\n",
    "# 원본 이미지 로드                              \n",
    "original_image = cv2.imread(image_path)\n",
    "original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 감지된 객체 후처리 및 시각화\n",
    "result_image = process_output(output_data, original_image_rgb)\n",
    "\n",
    "# 결과 이미지 출력\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "original_image = cv2.imread(image_path)\n",
    "original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "result_image = process_output(output_data, original_image_rgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
